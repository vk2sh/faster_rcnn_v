{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943fb963-ccd3-4b38-a623-9e54b64cb41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00583112-1204-4c70-81c9-e7647678fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "class CocoTransform:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)  \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a92e11d-988a-4c48-8d9c-7f076ce8b043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Dataset class\n",
    "def get_coco_dataset(img_dir, ann_file):\n",
    "    return CocoDetection(\n",
    "        root=img_dir,\n",
    "        annFile=ann_file,\n",
    "        transforms=CocoTransform()\n",
    "    )\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = get_coco_dataset(\n",
    "    img_dir=\"dataset/train\",\n",
    "    ann_file=\"dataset/train/annotation/demo_annotation_coco.json\"\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = get_coco_dataset(\n",
    "    img_dir=\"dataset/val\",\n",
    "    ann_file=\"dataset/val/annotation/demo_annotation_coco.json\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdeca528-ee26-49e0-b237-92ccd847741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Faster R-CNN with ResNet-50 backbone\n",
    "def get_model(num_classes):\n",
    "    # Load pre-trained Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e7e2f-4a6b-4ec9-868b-6ad10b1915f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cb2225-4fcf-4e28-a486-90468382788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\faster_rcnn_v\\myvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\faster_rcnn_v\\myvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_classes = 17 # Background + chair, human, table\n",
    "model = get_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78078c47-f5ce-47cb-b253-93d983074b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1975f5-6fcf-409c-8a92-c0becccc1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    for images, targets in data_loader:\n",
    "        # Move images to the device\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        # Validate and process targets\n",
    "        processed_targets = []\n",
    "        valid_images = []\n",
    "        for i, target in enumerate(targets):\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            for obj in target:\n",
    "                # Extract bbox\n",
    "                bbox = obj[\"bbox\"]  # Format: [x, y, width, height]\n",
    "                x, y, w, h = bbox\n",
    "\n",
    "                # Ensure the width and height are positive\n",
    "                if w > 0 and h > 0:\n",
    "                    boxes.append([x, y, x + w, y + h])  # Convert to [x_min, y_min, x_max, y_max]\n",
    "                    labels.append(obj[\"category_id\"])\n",
    "\n",
    "            # Only process if there are valid boxes\n",
    "            if boxes:\n",
    "                processed_target = {\n",
    "                    \"boxes\": torch.tensor(boxes, dtype=torch.float32).to(device),\n",
    "                    \"labels\": torch.tensor(labels, dtype=torch.int64).to(device),\n",
    "                }\n",
    "                processed_targets.append(processed_target)\n",
    "                valid_images.append(images[i])  # Add only valid images\n",
    "\n",
    "        # Skip iteration if no valid targets\n",
    "        if not processed_targets:\n",
    "            continue\n",
    "\n",
    "        # Ensure images and targets are aligned\n",
    "        images = valid_images\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, processed_targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch}] Loss: {losses.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6c7174-d337-41b2-959a-140c2a974bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] Loss: 1.4751\n",
      "Model saved: fasterrcnn_resnet50_epoch_1.pth\n",
      "Epoch [1] Loss: 1.2533\n",
      "Model saved: fasterrcnn_resnet50_epoch_2.pth\n",
      "Epoch [2] Loss: 1.4668\n",
      "Model saved: fasterrcnn_resnet50_epoch_3.pth\n",
      "Epoch [3] Loss: 1.3975\n",
      "Model saved: fasterrcnn_resnet50_epoch_4.pth\n",
      "Epoch [4] Loss: 1.3090\n",
      "Model saved: fasterrcnn_resnet50_epoch_5.pth\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the model's state dictionary after every epoch\n",
    "    model_path = f\"fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20e50932-a6c8-4944-8018-a463e2e82758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAMWCAYAAADF5hp2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEkBJREFUeJzt10ENACAQwDDAv+dDBA+ypFWw7/bMzAIAAICo8zsAAAAAXhhbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAQJqxBQAAIM3YAgAAkGZsAQAASDO2AAAApBlbAAAA0owtAAAAacYWAACANGMLAABAmrEFAAAgzdgCAACQZmwBAABIM7YAAACkGVsAAADSjC0AAABpxhYAAIA0YwsAAECasQUAACDN2AIAAJBmbAEAAEgztgAAAKQZWwAAANKMLQAAAGnGFgAAgDRjCwAAwCq7DncKKOs0z8QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load Faster R-CNN with ResNet-50 backbone\n",
    "def get_model(num_classes):\n",
    "    # Load pre-trained Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 17  # Background + chair + person + table\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model = get_model(num_classes)\n",
    "model.load_state_dict(torch.load(\"fasterrcnn_resnet50_epoch_5.pth\"))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "    return image_tensor.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Load the unseen image\n",
    "image_path = \"test.jpg\"\n",
    "image_tensor = prepare_image(image_path)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    prediction = model(image_tensor)\n",
    "\n",
    "# `prediction` contains:\n",
    "# - boxes: predicted bounding boxes\n",
    "# - labels: predicted class labels\n",
    "# - scores: predicted scores for each box (confidence level)\n",
    "COCO_CLASSES = { 1: \"Alisa\",\n",
    "    2: \"Kit CAESAR\", \n",
    "    3: \"Digestive wheat biscuits\", \n",
    "    4: \"Sunflower Seed Breed \",\n",
    "    5: \"Digestive\",\n",
    "    6: \"Rich Tea \",\n",
    "    7: \"lays\",\n",
    "    8: \"Quadratini\",\n",
    "    9: \"japanes rice cake \",\n",
    "    10: \"Choceur\",\n",
    "    11: \"Victoria\",\n",
    "    12: \"extra virgin olive oil\",\n",
    "    13: \"Indo mie\",\n",
    "    14: \"Theraflu\",\n",
    "    15: \"hearts of palms\",\n",
    "    16: \"Pickled sliced beets\"}\n",
    "def get_class_name(class_id):\n",
    "    return COCO_CLASSES.get(class_id, \"Unknown\")\n",
    "    \n",
    "# Draw bounding boxes with the correct class names and increase image size\n",
    "def draw_boxes(image, prediction, fig_size=(10, 10)):\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()  # Get predicted bounding boxes\n",
    "    labels = prediction[0]['labels'].cpu().numpy()  # Get predicted labels\n",
    "    scores = prediction[0]['scores'].cpu().numpy()  # Get predicted scores\n",
    "    \n",
    "    threshold = 0.5\n",
    "    plt.figure(figsize=fig_size)  \n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            class_name = get_class_name(label)  # Get the class name\n",
    "            plt.imshow(image)  # Display the image\n",
    "            plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                              linewidth=2, edgecolor='r', facecolor='none'))\n",
    "            plt.text(x_min, y_min, f\"{class_name} ({score:.2f})\", color='r')\n",
    "    \n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "\n",
    "# Display the image with bounding boxes and correct labels\n",
    "draw_boxes(Image.open(image_path), prediction, fig_size=(12, 10))  # Example of increased size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca03813-ade4-4a5e-a4c1-d8590e591272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 86\n",
      "[0] Label: 15, Score: 0.0800, Box: [ 587.28766   70.3323  1089.7932   950.1751 ]\n",
      "[1] Label: 7, Score: 0.0732, Box: [ 527.75464  147.07712 1043.467   1031.988  ]\n",
      "[2] Label: 15, Score: 0.0728, Box: [1115.7904   146.86984 1593.413   1018.50116]\n",
      "[3] Label: 9, Score: 0.0715, Box: [1042.2179   171.02885 1623.8121   878.31726]\n",
      "[4] Label: 9, Score: 0.0682, Box: [  9.388274 154.11542  525.3043   848.9764  ]\n",
      "[5] Label: 15, Score: 0.0661, Box: [ 153.48618  174.13509  569.39856 1007.1159 ]\n",
      "[6] Label: 10, Score: 0.0657, Box: [ 663.7674   462.66702 1074.3462   918.564  ]\n",
      "[7] Label: 3, Score: 0.0655, Box: [  5.2828426 297.836     635.33496   951.3196   ]\n",
      "[8] Label: 10, Score: 0.0653, Box: [1225.6284   195.29091 1630.3195   981.1929 ]\n",
      "[9] Label: 15, Score: 0.0646, Box: [  17.49702  277.90387  507.79016 1156.7455 ]\n",
      "[10] Label: 10, Score: 0.0637, Box: [244.55258 285.54416 536.99506 916.6909 ]\n",
      "[11] Label: 15, Score: 0.0636, Box: [1253.0557   362.73755 1577.2968   976.1737 ]\n",
      "[12] Label: 7, Score: 0.0633, Box: [ 12.611139 267.42847  632.56274  969.1075  ]\n",
      "[13] Label: 1, Score: 0.0631, Box: [1050.6785   195.16307 1634.       832.9091 ]\n",
      "[14] Label: 7, Score: 0.0629, Box: [1005.9395   258.65338 1631.3429  1027.8392 ]\n",
      "[15] Label: 3, Score: 0.0629, Box: [ 605.8596   236.75021 1072.1793   972.05023]\n",
      "[16] Label: 13, Score: 0.0613, Box: [ 510.28394  342.92953 1080.5193   934.2891 ]\n",
      "[17] Label: 6, Score: 0.0612, Box: [ 608.746    216.40808 1084.3864   994.934  ]\n",
      "[18] Label: 5, Score: 0.0611, Box: [ 620.43256  237.76213 1072.8411   982.766  ]\n",
      "[19] Label: 8, Score: 0.0609, Box: [170.00697 179.15897 579.7632  974.5445 ]\n",
      "[20] Label: 5, Score: 0.0608, Box: [  0.      187.97833 513.62964 839.48676]\n",
      "[21] Label: 13, Score: 0.0606, Box: [1083.9182  431.8253 1620.4685  928.3736]\n",
      "[22] Label: 1, Score: 0.0606, Box: [ 44.836815 417.03726  455.62408  867.19684 ]\n",
      "[23] Label: 5, Score: 0.0605, Box: [ 997.22107  282.8784  1624.3505   980.6921 ]\n",
      "[24] Label: 1, Score: 0.0602, Box: [ 591.8817   481.48615 1019.9853   904.1084 ]\n",
      "[25] Label: 9, Score: 0.0601, Box: [ 469.52533  155.05844 1106.55     886.9575 ]\n",
      "[26] Label: 13, Score: 0.0601, Box: [  0.      384.04495 468.8271  914.3229 ]\n",
      "[27] Label: 15, Score: 0.0600, Box: [ 77.576584 296.99753  384.38474  921.1984  ]\n",
      "[28] Label: 3, Score: 0.0600, Box: [ 992.22943  286.6975  1627.0901   986.31903]\n",
      "[29] Label: 14, Score: 0.0600, Box: [ 538.0412   256.22232 1210.7979   882.1114 ]\n",
      "[30] Label: 8, Score: 0.0600, Box: [ 623.39166  237.67215 1074.1129   962.5279 ]\n",
      "[31] Label: 10, Score: 0.0598, Box: [ 554.5495   239.43893 1174.8678   928.67834]\n",
      "[32] Label: 1, Score: 0.0598, Box: [1231.7423   432.51465 1634.       848.3599 ]\n",
      "[33] Label: 10, Score: 0.0597, Box: [ 552.9699   149.20844  928.9323  1093.0944 ]\n",
      "[34] Label: 7, Score: 0.0591, Box: [1109.0461   507.9313  1585.5509   977.18054]\n",
      "[35] Label: 15, Score: 0.0588, Box: [ 516.57153  415.96216 1005.5102  1188.365  ]\n",
      "[36] Label: 1, Score: 0.0587, Box: [ 576.8625   342.36935 1031.0754   767.8926 ]\n",
      "[37] Label: 15, Score: 0.0583, Box: [657.09314 393.31104 987.1784  966.19995]\n",
      "[38] Label: 15, Score: 0.0581, Box: [311.84512 322.12604 543.79156 951.4959 ]\n",
      "[39] Label: 1, Score: 0.0581, Box: [1110.8683  512.987  1591.2917  959.7345]\n",
      "[40] Label: 10, Score: 0.0577, Box: [ 44.5321  419.89697 432.55    883.5198 ]\n",
      "[41] Label: 15, Score: 0.0576, Box: [ 775.2771   358.8862  1054.4014   991.59973]\n",
      "[42] Label: 12, Score: 0.0573, Box: [ 630.83325  217.13869 1084.7069   995.8362 ]\n",
      "[43] Label: 15, Score: 0.0571, Box: [1184.4865  398.749  1634.     1219.3694]\n",
      "[44] Label: 1, Score: 0.0570, Box: [  0.      177.17079 529.1652  810.22174]\n",
      "[45] Label: 10, Score: 0.0566, Box: [1166.8284  472.2748 1492.3575  990.0369]\n",
      "[46] Label: 6, Score: 0.0566, Box: [ 990.42114  267.51587 1634.      1012.6948 ]\n",
      "[47] Label: 13, Score: 0.0563, Box: [1028.702      34.498978 1517.5704    864.4352  ]\n",
      "[48] Label: 16, Score: 0.0560, Box: [ 589.77936    27.423416 1102.337     923.13745 ]\n",
      "[49] Label: 10, Score: 0.0560, Box: [   9.504565  166.62575   413.91693  1026.7874  ]\n",
      "[50] Label: 3, Score: 0.0558, Box: [236.41124 290.86566 526.78394 882.201  ]\n",
      "[51] Label: 1, Score: 0.0557, Box: [246.28773 293.9679  545.2229  901.5932 ]\n",
      "[52] Label: 6, Score: 0.0556, Box: [150.00989 157.51021 586.33923 977.2418 ]\n",
      "[53] Label: 11, Score: 0.0552, Box: [ 615.7219   223.48792 1085.2072  1000.54517]\n",
      "[54] Label: 10, Score: 0.0552, Box: [549.05676 548.0712  968.1543  953.6307 ]\n",
      "[55] Label: 16, Score: 0.0550, Box: [  26.55848  294.2737   528.3893  1154.7278 ]\n",
      "[56] Label: 12, Score: 0.0547, Box: [ 168.05382  145.46399  589.3946  1006.2849 ]\n",
      "[57] Label: 8, Score: 0.0543, Box: [  2.600736 190.53468  422.20758  982.3907  ]\n",
      "[58] Label: 1, Score: 0.0543, Box: [ 615.12537  224.13933 1079.4819   970.5143 ]\n",
      "[59] Label: 11, Score: 0.0540, Box: [1084.154   419.4676 1627.8723  934.0438]\n",
      "[60] Label: 7, Score: 0.0538, Box: [ 38.613144 342.45267  372.55603  971.08673 ]\n",
      "[61] Label: 10, Score: 0.0537, Box: [1109.8577   483.07883 1614.7776   830.7512 ]\n",
      "[62] Label: 8, Score: 0.0535, Box: [  69.55792 1025.8855   372.89352 1192.9742 ]\n",
      "[63] Label: 7, Score: 0.0534, Box: [ 615.90546  510.7791  1060.6223   953.7607 ]\n",
      "[64] Label: 13, Score: 0.0531, Box: [  59.8578  1012.70105  341.9175  1226.     ]\n",
      "[65] Label: 15, Score: 0.0531, Box: [  0.        0.      432.23972 928.86414]\n",
      "[66] Label: 8, Score: 0.0530, Box: [1087.6378   420.43518 1627.2776   923.01306]\n",
      "[67] Label: 14, Score: 0.0530, Box: [  0.      347.77353 490.56467 915.7361 ]\n",
      "[68] Label: 16, Score: 0.0530, Box: [1121.485     108.552895 1612.0519   1010.83606 ]\n",
      "[69] Label: 14, Score: 0.0529, Box: [1031.6465   191.61156 1634.       844.62695]\n",
      "[70] Label: 2, Score: 0.0529, Box: [ 517.8586   146.72209 1032.9459  1058.6886 ]\n",
      "[71] Label: 2, Score: 0.0526, Box: [   0.       100.22007  559.9991  1090.7975 ]\n",
      "[72] Label: 12, Score: 0.0524, Box: [  41.04938  303.19418  538.0238  1163.6627 ]\n",
      "[73] Label: 1, Score: 0.0523, Box: [127.91011 516.1985  551.1961  922.4847 ]\n",
      "[74] Label: 10, Score: 0.0522, Box: [ 561.40814  337.4687  1009.8486   803.81067]\n",
      "[75] Label: 13, Score: 0.0521, Box: [ 539.3157    51.70158 1045.9725   821.2632 ]\n",
      "[76] Label: 11, Score: 0.0518, Box: [  0.     281.091  629.0496 966.1013]\n",
      "[77] Label: 10, Score: 0.0517, Box: [1307.2843   458.72928 1598.5281  1000.83026]\n",
      "[78] Label: 12, Score: 0.0517, Box: [1138.8911   305.98932 1634.      1078.8855 ]\n",
      "[79] Label: 13, Score: 0.0513, Box: [553.05585 564.64435 985.26337 936.9513 ]\n",
      "[80] Label: 7, Score: 0.0512, Box: [251.91734 271.9062  546.8326  932.8752 ]\n",
      "[81] Label: 16, Score: 0.0511, Box: [250.70871 244.21979 542.2439  918.82935]\n",
      "[82] Label: 4, Score: 0.0511, Box: [1045.6671  188.3124 1634.      846.8141]\n",
      "[83] Label: 2, Score: 0.0508, Box: [ 980.0794  268.0433 1614.7157 1026.774 ]\n",
      "[84] Label: 9, Score: 0.0506, Box: [1068.7677   516.78815 1621.6432   893.15344]\n",
      "[85] Label: 16, Score: 0.0501, Box: [1217.4022   489.56052 1618.9537   959.23346]\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Print predictions\n",
    "boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "labels = prediction[0]['labels'].cpu().numpy()\n",
    "scores = prediction[0]['scores'].cpu().numpy()\n",
    "\n",
    "print(\"Number of predictions:\", len(boxes))\n",
    "for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
    "    print(f\"[{i}] Label: {label}, Score: {score:.4f}, Box: {box}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
